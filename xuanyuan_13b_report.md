## 介绍
XuanYuan-13B是基于Llama2-13B模型进行中文增强的金融大模型，包含大量中英文语料增量预训练之后的底座模型以及使用高质量指令数据和强化学习进行对齐的chat模型。XuanYuan-13B的推出标志着智能金融领域的一次新的突破，它可以在日常对话、语言理解、知识运用、内容创作、信息摘要等方面与70B量级模型相媲美。在模型训练中，团队在模型预训练阶段动态调整不同语种与领域知识的比例，融入了大量的专业金融语料，并在指令微调中灵活运用之前提出的Self-QA和混合训练方法，显著提升了模型在对话中的性能表现。随着XuanYuan-13B的推出，金融领域的智能化水平将迈上一个新的台阶。它将为金融机构、投资者、研究人员等提供更准确、及时的信息，并帮助他们做出更明智的决策。同时，轩辕13B也将成为金融教育和普及的有力工具，为广大民众提供可靠的金融知识和建议。随着时间的推移，团队将继续不断改进和优化这个模型，以满足不断发展的金融领域需求。通过不断学习和适应，轩辕13B将成为一个真正可靠且强大的金融智能伙伴，为用户带来更多惊喜和价值。

## 词表初始化
由于Llama原始词表主要以英文为主，需要对新添加的中文词汇向量进行初始化。为了尽可能使新添加的中文词汇向量尽可能与原有词表对齐，不同与之前发布的模型，我们使用了均值法进行表示的初始化。

Examples：白日依山尽

原始词表分词后：['白', '日', '<0xE4>', '<0xBE>', '<0x9D>', '山', '<0xE5>', '<0xB0>', '<0xBD>']
扩充词表分词后：['白', '日', '依', '山', '尽']

向量表示：对于已存在于原词表的中文token则直接使用原始token，而对于新增的中文token，如果是单个字则使用其拆分到元词表的多个向量平均值组合作为新的向量表示，如果是多个字则拆分成单个字后按照单个字处理：

比如：【依】在原词表向量表示为【'<0xE4>', '<0xBE>', '<0x9D>'】，则新的原始表示为这三个token向量的平均值；
【依尽】这个词拆分成【依】和【依】，进而转换为原词表的【'<0xE4>', '<0xBE>', '<0x9D>', '<0xE5>', '<0xB0>', '<0xBD>'】，再取这六个token的均值作为新词的表示。

## 预训练数据准备
我们整理了如下来源的数据，并按以下的配比进行数据的采样和组合，包括了全网大部分中文数据。对于这些数据除了最开始的粗粒度预处理和去重外，我们还对文本进行了更细致的处理，并将最终的预训练数据进行分词后输入到模型中。

<img src="resources/13b3.png"  width="31%" />

## SFT数据构建

数据层面提出了开创性的指令微调数据全新收集方法self-QA[^Self-QA]，可以在没有人工标注的情况下生成大量高质量有监督的问答数据。该方法的基本思想是通过利用已有的高质量大模型根据无监督的知识生成有监督的微调数据。这些无监督的知识可以是书籍或网页，也可以是从结构化的表格数据或图谱数据中转换而成的非结构化文档数据。总的来说，数据优化流水线可以分为以下三个阶段：

（1）知识引导的指令生成：在这个阶段，使用语言模型ChatGPT根据无监督文本生成指令。这种方法使得生成的指令与领域相关，并且与提供的无监督文本内容相关。然而在训练和推理的过程中，指令是在没有背景知识的情况下提供给语言模型的，所以需要提供一些准则，使得这些指令不能依赖于和参考原始文本中的内容。
然后就可以获得几个相关的指令，这些指令可以在下一个阶段中使用。提示中的无监督知识数据代表连续文本。无结构化知识，如网页和书籍数据，可以在经过预处理后直接使用。结构化数据，如表格和知识图谱，在使用之前需要转换为无结构化的文本数据，可以通过使用模板填充槽或将每个数据条目与其相应的属性名连接起来来实现。

（2）机器阅读理解：在这个阶段，语言模型需要根据相应的无监督知识对生成的指令问题生成答案。由于整个过程与阅读理解的过程相同，所以也称这个阶段为阅读理解。

（3）修剪与过滤：尽管在上面明确指示模型在生成的问题中不要假设外部文档的先验知识，并禁止使用“这个”这样的指示代词来生成问题，以及在生成的答案中不包含“根据以上内容”等短语，但仍然有语言模型生成的文本违反了这些规则。此外，生成的指令示例也存在不符合所需格式且无法解析的情况。因此，有必要进一步过滤掉这些有问题的示例。通过应用不同的启发式过滤器，确保生成的文本符合预定义的准则，并保持所需的正确性和连贯性水平。之后便可以直接使用生成的问题和答案作为指令调优数据。此外，还可以将指令和相应的无监督知识添加到问题中，使模型能够学习定制的领域任务，例如阅读理解和信息提取。
通过以上步骤，Self-QA方法便能够在缺乏指令数据的情况下，利用高质量的大模型从无监督的文档中生成指令数据，并通过有监督微调提高模型在指令遵循方面的能力。

## 模型训练

提出了全新的微调方法——混合微调训练框架[^Hybrid-tuning]，改变了ChatGPT提出的指令微调训练方式，是训练方式的全新里程碑开启了大模型混合训练的全新时代。虽然通用大型模型的使用更加广泛，但领域特定模型的重要性也是不容忽视。在许多领域中，语言的分布和特定的语言细微差别需要针对特定领域进行微调或专门训练的模型。因此，已经有了一系列特定领域的大型模型，以满足各个领域的独特需求。另外，特定领域的语言模型和聊天模型与一般领域模型相比对数据分发和训练方法提出了更高的要求。特定领域模型往往需要捕捉特定领域的独特语言特征、术语和上下文，以实现最佳性能。然而，仅根据特定领域的数据训练这些模型可能会导致灾难性的遗忘，即模型失去了以前从一般领域学到的知识，从而影响其整体性能。
为了缓解这个问题，提出了一种新的训练策略，混合微调。

混合微调方法可以在微调阶段很好地融合无监督的预训练数据（包括通用预训练数据和特定领域的预训练数据）和有监督的指令微调数据（包括通用指令数据和领域指令数据），从而避免灾难性遗忘现象的发生。对于无监督的预训练数据，可以从互联网上抓取并清理和过滤它们。对于有监督的指令调整数据，我们使用之前提到的Self instruction和Self-QA方法来进行收集。
混合微调方法的优势在于，它能够充分利用预训练模型在大规模无监督数据上学到的语言表示能力，同时通过有监督的指令微调数据提供任务特定的指导。通过将无监督数据和有监督数据进行混合，通用和特定领域混合，模型可以在微调过程中保持对预训练知识的记忆，避免灾难性遗忘现象的发生。这种方法不仅可以提高模型在特定任务上的性能，还能够提升模型的泛化能力和适应性。


## RLHF模型训练
除了中文增强预训练和指令微调外，我们还进行了RLHF。通过人类反馈的强化学习(RLHF)训练，提高与人类偏好进行对齐的Chat模型。相比于原始模型，RLHF对齐后的模型，在文本创作、内容生成 、指令理解与遵循、安全性等方面都有较大的提升。我们参考InstructGPT[^instructgpt]和LLaMA2[^llama2]的方法，实现了RLHF方法来使用人类偏好标注数据来进一步提高SFT模型的对齐程度。这个过程主要包含了奖励模型(Reward Model)的训练来学习人类对于当前模型的输出结果的偏好情况，以及Proximal Policy Optimization (PPO) 来进行策略学习。

### Reward Model training
为了训练有效的Reward Model来进行RLHF对齐，偏好数据的质量是非常重要的，我们在构造pair标注数据时候，基于SFT模型采用了不同的采样超参数生成出多组采样回复，并且从中选择多样性较大的回复来组成偏好标注pair；为了进一步增加偏好数据，我们进一步补充了部分由更大参数量模型生成回复来组成标注pair，这样做的目的是希望随着PPO的训练，Reward Model能够缓解部分由于actor distribution shift带来奖励分布偏移问题。在训练过程中，我们使用SFT Model作为Reward Model的参数初始化，去掉最后一层causal layer的同时，增加一层随机初始化的value layer来估计[prompt, response]的评分。我们在chosen和rejected response之后加入了special token，来指示回复的结束，并且通过计算token-level的对比损失函数来训练Reward Model。

### PPO training
PPO训练中包含了四个模型，我们使用Xuanyuan-13B-SFT模型作为actor和reference model，采用13B-SFT模型训练的Reward Model来作为Reward Model和Critic Model的初始化。
在PPO的训练过程中，我们使用Temperature=0.7，Top_p=0.9来增加训练过程中探索生成response的多样性，这个策略有助于提升模型训练结果的稳定性。在训练过程中，我们使用1%的SFT模型的学习率来训练actor和critic model。我们将KL Reward系数设置为0.05并且标准化Reward。实验发现，过大的KL Reward系数会导致模型难以训练，而过小的KL Reward系数会导致模型容易出现reward hacking现象，从而倾向于生成更长的回复。

## 模型评测

### 通用评测

我们将XuanYuan-13B在通用评测中的绝大部分指标：都可以与其他开源系列的70B左右参数模型相媲美，体现了轩辕13B非常强大的能力，主要评测指标如下：

<img src="resources/13b1.png"  width="71%" />

### 金融评测

XuanYuan-13B在金融评测也表现出极高的水平，以小博大获得了很好的表现。金融的主要评测指标如下：

<img src="resources/13b2.png"  width="75%" />

[^Self-QA]:Xuanyu, Zhang, et al. "Self-QA: Unsupervised Knowledge Guided Language Model Alignment" _arXiv preprint arXiv:2305.11952_ (2023).
[^Hybrid-tuning]:Xuanyu, Zhang, et al. "XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters" _CIKM_ (2023): 4435-4439.
[^instructgpt]:Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.
[^anthropic]:Bai, Yuntao, et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." _arXiv preprint arXiv:2204.05862_ (2022).
[^llama2]:Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." _arXiv preprint arXiv:2307.09288_ (2023).
