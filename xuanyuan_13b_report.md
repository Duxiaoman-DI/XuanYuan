## 介绍
XuanYuan-13B是基于Llama2-13B模型进行中文增强的金融大模型，包含大量中英文语料增量预训练之后的底座模型以及使用高质量指令数据和强化学习进行对齐的chat模型。XuanYuan-13B的推出标志着智能金融领域的一次新的突破，它可以在日常对话、语言理解、知识运用、内容创作、信息摘要等方面与70B量级模型相媲美。在模型训练中，团队在模型预训练阶段动态调整不同语种与领域知识的比例，融入了大量的专业金融语料，并在指令微调中灵活运用之前提出的Self-QA和混合训练方法，显著提升了模型在对话中的性能表现。随着XuanYuan-13B的推出，金融领域的智能化水平将迈上一个新的台阶。它将为金融机构、投资者、研究人员等提供更准确、及时的信息，并帮助他们做出更明智的决策。同时，轩辕13B也将成为金融教育和普及的有力工具，为广大民众提供可靠的金融知识和建议。随着时间的推移，团队将继续不断改进和优化这个模型，以满足不断发展的金融领域需求。通过不断学习和适应，轩辕13B将成为一个真正可靠且强大的金融智能伙伴，为用户带来更多惊喜和价值。

## 词表初始化
由于Llama原始词表主要以英文为主，需要对新添加的中文词汇向量进行初始化。为了尽可能使新添加的中文词汇向量尽可能与原有词表对齐，不同与之前发布的模型，我们使用了均值法进行表示的初始化。

Examples：白日依山尽

原始词表分词后：['白', '日', '<0xE4>', '<0xBE>', '<0x9D>', '山', '<0xE5>', '<0xB0>', '<0xBD>']
扩充词表分词后：['白', '日', '依', '山', '尽']

向量表示：对于已存在于原词表的中文token则直接使用原始token，而对于新增的中文token，如果是单个字则使用其拆分到元词表的多个向量平均值组合作为新的向量表示，如果是多个字则拆分成单个字后按照单个字处理：

比如：【依】在原词表向量表示为【'<0xE4>', '<0xBE>', '<0x9D>'】，则新的原始表示为这三个token向量的平均值；
【依尽】这个词拆分成【依】和【依】，进而转换为原词表的【'<0xE4>', '<0xBE>', '<0x9D>', '<0xE5>', '<0xB0>', '<0xBD>'】，再取这六个token的均值作为新词的表示。

## 预训练数据准备
我们整理了如下来源的数据，并按以下的配比进行数据的采样和组合，包括了全网大部分中文数据。对于这些数据除了最开始的粗粒度预处理和去重外，我们还对文本进行了更细致的处理，并将最终的预训练数据进行分词后输入到模型中。
<!--![13B数据](https://gitlab.duxiaoman-int.com/di-dxm/xuanyuan/raw/master/13b3.png)-->
<img src="https://gitlab.duxiaoman-int.com/di-dxm/xuanyuan/raw/master/13b3.png"  width="30%" />

## SFT数据构建

数据层面提出了开创性的指令微调数据全新收集方法self-QA，可以在没有人工标注的情况下生成大量高质量有监督的问答数据。该方法的基本思想是通过利用已有的高质量大模型根据无监督的知识生成有监督的微调数据。这些无监督的知识可以是书籍或网页，也可以是从结构化的表格数据或图谱数据中转换而成的非结构化文档数据。总的来说，数据优化流水线可以分为以下三个阶段：

（1）知识引导的指令生成：在这个阶段，使用语言模型ChatGPT根据无监督文本生成指令。这种方法使得生成的指令与领域相关，并且与提供的无监督文本内容相关。然而在训练和推理的过程中，指令是在没有背景知识的情况下提供给语言模型的，所以需要提供一些准则，使得这些指令不能依赖于和参考原始文本中的内容。
然后就可以获得几个相关的指令，这些指令可以在下一个阶段中使用。提示中的无监督知识数据代表连续文本。无结构化知识，如网页和书籍数据，可以在经过预处理后直接使用。结构化数据，如表格和知识图谱，在使用之前需要转换为无结构化的文本数据，可以通过使用模板填充槽或将每个数据条目与其相应的属性名连接起来来实现。

（2）机器阅读理解：在这个阶段，语言模型需要根据相应的无监督知识对生成的指令问题生成答案。由于整个过程与阅读理解的过程相同，所以也称这个阶段为阅读理解。

（3）修剪与过滤：尽管在上面明确指示模型在生成的问题中不要假设外部文档的先验知识，并禁止使用“这个”这样的指示代词来生成问题，以及在生成的答案中不包含“根据以上内容”等短语，但仍然有语言模型生成的文本违反了这些规则。此外，生成的指令示例也存在不符合所需格式且无法解析的情况。因此，有必要进一步过滤掉这些有问题的示例。通过应用不同的启发式过滤器，确保生成的文本符合预定义的准则，并保持所需的正确性和连贯性水平。之后便可以直接使用生成的问题和答案作为指令调优数据。此外，还可以将指令和相应的无监督知识添加到问题中，使模型能够学习定制的领域任务，例如阅读理解和信息提取。
通过以上步骤，Self-QA方法便能够在缺乏指令数据的情况下，利用高质量的大模型从无监督的文档中生成指令数据，并通过有监督微调提高模型在指令遵循方面的能力。

## 模型训练

提出了全新的微调方法——混合微调训练框架，从根本上改变了从2018年BERT提出到现在延用5年的预训练-微调的训练方式，也改变了ChatGPT提出的指令微调训练方式，是训练方式的全新里程碑，开启了大模型混合训练的全新时代。虽然通用大型模型的使用更加广泛，但领域特定模型的重要性也是不容忽视。在许多领域中，语言的分布和特定的语言细微差别需要针对特定领域进行微调或专门训练的模型。因此，已经有了一系列特定领域的大型模型，以满足各个领域的独特需求。另外，特定领域的语言模型和聊天模型与一般领域模型相比对数据分发和训练方法提出了更高的要求。特定领域模型往往需要捕捉特定领域的独特语言特征、术语和上下文，以实现最佳性能。然而，仅根据特定领域的数据训练这些模型可能会导致灾难性的遗忘，即模型失去了以前从一般领域学到的知识，从而影响其整体性能。
为了缓解这个问题，提出了一种新的训练策略，混合微调。

混合微调方法可以在微调阶段很好地融合无监督的预训练数据（包括通用预训练数据和特定领域的预训练数据）和有监督的指令微调数据（包括通用指令数据和领域指令数据），从而避免灾难性遗忘现象的发生。对于无监督的预训练数据，可以从互联网上抓取并清理和过滤它们。对于有监督的指令调整数据，我们使用之前提到的Self instruction和Self-QA方法来进行收集。
混合微调方法的优势在于，它能够充分利用预训练模型在大规模无监督数据上学到的语言表示能力，同时通过有监督的指令微调数据提供任务特定的指导。通过将无监督数据和有监督数据进行混合，通用和特定领域混合，模型可以在微调过程中保持对预训练知识的记忆，避免灾难性遗忘现象的发生。这种方法不仅可以提高模型在特定任务上的性能，还能够提升模型的泛化能力和适应性。

## 模型评测

我们将XuanYuan-13B在通用评测中的绝大部分指标都可以与72B相媲美，甚至是在Agent能力中远超72B，主要评测指标如下：
<img src="resources/13b1.png"  width="70%" />


XuanYuan-13B在金融评测也表现出极高的水平，媲美通义千问的72B模型，以小博大获得了很好的表现。金融的主要评测指标如下：
![13B金融](resources/13b2.png)

